{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - SQL\n",
    "\n",
    "In this assignment we are going to explore two public datasets from Google BigQuery. Similar to week 4 we will connect to BigQuery via Python. Your job is to construct SQL queris that answers to fullfil what is asked in the questions.\n",
    "\n",
    "__Submission:__ Please upload your notebook in moodle by __April 21st, 23:59__ (https://moodle.unil.ch/mod/assign/view.php?id=798095)\n",
    "\n",
    "__Grading:__ Later we will release a quiz regarding this assignment. Your quiz grade will be the grade of your assignment. (Essentially when you do the assignment, answering to the quiz questions would be very easy. this is just a way for grading the assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for colab users\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "print('Authenticated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a \"Client\" object\n",
    "client = bigquery.Client(project=\"week4-bigscaleanalytics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Github activity dataset\n",
    "\n",
    "GitHub is how people build software and is home to the largest community of open source developers in the world, with over 12 million people contributing to 31 million projects on GitHub since 2008.\n",
    "\n",
    "This 3TB+ dataset comprises the largest released source of GitHub activity to date. It contains a full snapshot of the content of more than 2.8 million open source GitHub repositories including more than 145 million unique commits, over 2 billion different file paths, and the contents of the latest revision for 163 million files, all of which are searchable with regular expressions.\n",
    "\n",
    "This public dataset is hosted in Google BigQuery and is included in BigQuery's 1TB/mo of free tier processing. This means that each user receives 1TB of free BigQuery processing every month, which can be used to run queries on this public dataset.\n",
    "\n",
    "Given the huge size of this dataset, performing a query on the GitHub Archive dataset can easily exceed your 1TB of processed data, which could exceed your free monthly quota. Therefore throughout this assignment we will always run queries on sample tables which are small samples of the data. Nevertheless, you should try to avoid queries which have a huge output. Always remember to use limit (especially if you are not sure about the output of your query) to limit the size of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a reference to the github repos dataset\n",
    "github_ref = client.dataset(\"github_repos\", project=\"bigquery-public-data\")\n",
    "\n",
    "# API request - fetch the dataset\n",
    "github_dataset = client.get_dataset(github_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a reference to the \"sample_repos\" table\n",
    "repos_ref = github_ref.table(\"languages\")\n",
    "\n",
    "# API request - fetch the table\n",
    "repos_table = client.get_table(repos_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repos_table.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- What are the 5 mostly used languages?\n",
    "\n",
    "__Hint: you may use the `language` and `sample_repos` tables. Note that the language column has a specific type. It is an array of struct. Refer to the big query documentation to know more about `ARRAY`and `STRUCT` data types. Essentialy here you have to extract the first element of the array using `OFFSET` function to get the struct. Then you can extract the language using the field `name` in the struct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "query_job_1 = client.query(q1)\n",
    "query_job_1.to_dataframe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Whats is the most popular repository which is mainly in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2 = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "query_job_2 = client.query(q2)\n",
    "query_job_2.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 - how many collaborators where involved in the \"tensorflow/tensorflow\t\"repository? return all of them in the decreasing order of number of commits they have authored.\n",
    "\n",
    "__Hint: you should use the author column from sample_contents table (you may want to check out the difference between an author and committer.)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the top 5 states with the most number of drunk drivers\n",
    "q3 = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "query_job_3 = client.query(q3)\n",
    "query_job_3.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4- For the person in the second row of the above table, Vijay Vasudevan, find the months in which he had the most number of commits in the tensorflow project. Return the numbers in descending format. (you need to return the month and the year)\n",
    "\n",
    "__Hint: check the `FORMAT_DATE` function to extract the month from a date column__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q4 = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "query_job_4 = client.query(q4)\n",
    "query_job_4.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5- list the top 5 people in the tensorflow Github who has the highest average length of commit messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q5 = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "query_job_5 = client.query(q5)\n",
    "query_job_5.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6- Find the commits in the tensorflow Github which were concerning \"cuda\" issues. Return the author name and the commit message and subject.\n",
    "\n",
    "__Hint: use `like` keyword to see if cuda appears in the commit message.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q6 = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "query_job_6 = client.query(q6)\n",
    "query_job_6.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7- You can find the language of the code scripts by looking at the extensions in the file names. For instance a `.py` file is a Python script. Find the 10 most used programming langages in the tensorflow repository by using the file extensions. Do you think using the file extensions is an efficient way to find the code scripts?\n",
    "\n",
    "__Hint: use the `sample_files` table. Essentially you need to extract what is comming after the dot(.) in a file name. For that you may use this regular expression: `REGEXP_EXTRACT(path, '[^.]+$')`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q7 = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "query_job_7 = client.query(q7)\n",
    "query_job_7.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google anlytics data\n",
    "\n",
    "The dataset provides 12 months (August 2016 to August 2017) of obfuscated Google Analytics 360 data from the Google Merchandise Store , a real ecommerce store that sells Google-branded merchandise, in BigQuery.\n",
    "\n",
    "The data is typical of what an ecommerce website would see and includes the following information:\n",
    "\n",
    "- Traffic source data: information about where website visitors originate, including data about organic traffic, paid search traffic, and display traffic\n",
    "- Content data: information about the behavior of users on the site, such as URLs of pages that visitors look at, how they interact with content, etc.\n",
    "- Transactional data: information about the transactions on the Google Merchandise Store website.\n",
    "\n",
    "This public dataset is hosted in Google BigQuery and is included in BigQuery's 1TB/mo of free tier processing. This means that each user receives 1TB of free BigQuery processing every month, which can be used to run queries on this public dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains a separtate table for each month. It is possible to run queries on multiple tables to do some aggragation, etc on several month. This could be done by `_TABLE_SUFFIX` keyword. Look at the example query below to see how this could be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example query\n",
    "q_example = \"\"\"\n",
    "select * from \n",
    "`bigquery-public-data.google_analytics_sample.ga_sessions_*`\n",
    "  WHERE\n",
    "    _TABLE_SUFFIX BETWEEN '20170701'\n",
    "    AND '20170731'\n",
    "limit 10\n",
    "\"\"\"\n",
    "\n",
    "query_job_ex = client.query(q_example)\n",
    "query_job_ex.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8- Find the average of total transactions per user for July 2017.\n",
    "\n",
    "__Hint: again here note that there are some struct and array columns. For instance for this question we need the number of transactions for each row (a purchase of a user). For this we have to extract `transactions` from `totals` column i.e, `totals.transaction` __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_8 = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "query_job_9 = client.query(q_9)\n",
    "query_job_9.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9- find the most used browser among the users in Northern America for July 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_9 = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "query_job_10 = client.query(q_10)\n",
    "query_job_10.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10- List the top 10 regions in United Stated with the highest average revenue per transaction. \n",
    "\n",
    "__Hint: use the `region` field from `geoNetwork` column (`geoNetwork.region`).__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q10 = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "query_job_10 = client.query(q10)\n",
    "query_job_10.to_dataframe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
